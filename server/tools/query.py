import logging

from mcp_instance import mcp
from rag.qa_chain import qa_chain
from rag.retrieval import get_retriever
from utils.log_time import log_time
from utils.config import load_env_vars
from langchain_community.llms import Ollama
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

config = load_env_vars()

@log_time
@mcp.tool()
def query(query: str, identifier: str, use_web_search: bool = False) -> str:
    """
    Query the indexed documents (and optionally the web) to find the most relevant answer to the user's question.
    Only document with the matching identifier will be considered
    
    Args:
        query (str): The user's query.
        identifier (str): Identifier for the document store.
        use_web_search (bool): Whether to use web search for additional context.
        
    Returns:
        str: The final answer generated by the LLM, or web search results if no local documents are found and web search is enabled.
    """
    logging.info(f"\n========================================================================================\n")
    logging.info(f"Query tool called with query: {query}, identifier: {identifier}, (use_web_search={use_web_search})")
    
    embeddings = OllamaEmbeddings(model=config['EMBEDDING_MODEL']) 
    vectorstore = Chroma(persist_directory=config['VECTORSTORE_DIR'], embedding_function=embeddings)
    logging.info(f"Loaded vectorstore from {config['VECTORSTORE_DIR']} with {vectorstore._collection.count()} documents.")

    metadata_filter = {"identifier": identifier}
    retriever = get_retriever(vectorstore, top_k=config['TOP_K'], metadata_filter=metadata_filter)
    logging.info(f"Created retriever with metadata filter: {metadata_filter}")

    # Retrieve relevant documents
    retriever_docs = retriever.get_relevant_documents(query)
    for doc in retriever_docs:
        logging.info(f"Retrieved chunk:\n {doc.page_content} \n")
    
    logging.info(f"Retriever ready")
    llm = Ollama(model=config['LLM_MODEL'])
    logging.info(f"LLM model loaded: {config['LLM_MODEL']}")

    # Pass to QA chain
    answer = qa_chain(query, retriever, llm, use_web_search=use_web_search)
    logging.info(f"LLM systhesized answer: {answer['results']}")

    if not answer['results']:
        logging.info("No relevant documents found and web search is disabled.")
        logging.info(f"\n========================================================================================\n")
        return "No relevant documents found and web search is disabled."

    logging.info(f"\n========================================================================================\n")
    return f"Source: {answer['source']}\nAnswer: {answer['results']}"